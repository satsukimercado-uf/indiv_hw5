{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec746dd7",
   "metadata": {},
   "source": [
    "Design and train (with code or manually) a neural network that achieves best possible accuracy on the attached training_data.csv file. Discuss your design choices. In particular, describe your architecture (i.e., depth = number of layers and width = number of units per layer), training method, and hyperparameters. State the number of trainable parameters in your network, and report the final accuracy on the test set, located in file test_data.csv. Submit your code on GitHub Classroom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b230da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data split features and labels \n",
    "df = pd.read_csv(\"training_data.csv\")\n",
    "data = np.array(df)\n",
    "x = data[:,:-1]\n",
    "y = data[:, -1:]\n",
    "\n",
    "# Train validate split \n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size= 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca451e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network class \n",
    "class NeuralNetMLP:\n",
    "\n",
    "    # Initialize the weights and bias term vectors \n",
    "    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):\n",
    "        # Initialize values\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Hidden layers\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        self.weight_h = rng.normal(loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "\n",
    "        # Output layer\n",
    "        self.weight_out = rng.normal(loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "\n",
    "    # Forward pass, take an example and make a prediction \n",
    "    def forward(self, x):\n",
    "        # Hidden layer output z\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        # Hidden layer activation \n",
    "        a_h = relu(z_h)\n",
    "\n",
    "        # Output layer output z \n",
    "        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n",
    "        # Output layer activation \n",
    "        a_out = relu(z_out)\n",
    "        return a_h, a_out\n",
    "    \n",
    "    def backward(self, x, a_h, a_out, y):  \n",
    "        # x = input layer\n",
    "        # a_h = activation for hidden layers\n",
    "        # a_out = output of activation \n",
    "        # y = true labels \n",
    "\n",
    "        # Part 1: dLoss/dOutWeights\n",
    "\n",
    "        # d loss / d output activation\n",
    "        d_loss__d_a_out = 2.*(a_out - y) / y.shape[0]\n",
    "\n",
    "        # d output activation / d output z \n",
    "        d_a_out__d_z_out = a_out * (1. - a_out) \n",
    "\n",
    "        # change in output = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule) placeholder\"\n",
    "\n",
    "        # Gradient for output weights \n",
    "        d_z_out__dw_out = a_h\n",
    "        \n",
    "        # d loss/ d output w\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "        # d loss/ d output bias term \n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "               \n",
    "        # Part 2: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "        \n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__a_h = self.weight_out\n",
    "        \n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h * (1. - a_h) # sigmoid derivative\n",
    "        \n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "        \n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out, \n",
    "                d_loss__d_w_h, d_loss__d_b_h)\n",
    "\n",
    "\n",
    "# Write ReLU activation function\n",
    "def relu(x):\n",
    "    # ReLU = max{0, x}\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that actually runs the neural network\n",
    "def train(model, x_train, y_train, x_valid, y_valid, num_epochs,learning_rate=0.1):\n",
    "    \n",
    "    # Lists to store these values after each epoch\n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # iterate over minibatches\n",
    "        minibatch_gen = minibatch_generator(\n",
    "            X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "            \n",
    "            #### Compute outputs ####\n",
    "            a_h, a_out = model.forward(X_train_mini)\n",
    "\n",
    "            #### Compute gradients ####\n",
    "            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = \\\n",
    "                model.backward(X_train_mini, a_h, a_out, y_train_mini)\n",
    "\n",
    "            #### Update weights ####\n",
    "            model.weight_h -= learning_rate * d_loss__d_w_h\n",
    "            model.bias_h -= learning_rate * d_loss__d_b_h\n",
    "            model.weight_out -= learning_rate * d_loss__d_w_out\n",
    "            model.bias_out -= learning_rate * d_loss__d_b_out\n",
    "        \n",
    "        #### Epoch Logging ####        \n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
